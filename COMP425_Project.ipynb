{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "COMP425_Project.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cgreczkowski/Comp425_Project/blob/master/COMP425_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlun5t_GO2Xi"
   },
   "source": [
    "# COMP425/6341 Computer Vision - Project skeleton\n",
    "\n",
    "## 0 Import KMNIST dataset (1 pts)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a4G0UJ4EcIfX"
   },
   "source": [
    "# import required libraries, DO NOT MODIFY!\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder,KMNIST\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5ADRTRFNcIfY"
   },
   "source": [
    "### TODO: set random seed to your Student ID\n",
    "random_seed = 40001600\n",
    "torch.manual_seed(random_seed);"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdFR_tBZKAiR"
   },
   "source": [
    "Run the cells below to initialize the train and test loaders for KMNIST and visualize one of its samples.\n",
    "\n",
    "**Experiments:** <br>\n",
    "1. Change `batch_size` based on the device you are using.<br>\n",
    "2. Try more complicated transformations on train set.<br>\n",
    "3. Visualize different samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y9UQampoKAiS"
   },
   "source": [
    "# datasets hyper parameters\n",
    "batch_size = 20\n",
    "train_transform = transforms.Compose([transforms.ToTensor()])\n",
    "test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Initialize kmnist train and test datasets\n",
    "# These two lines will download the datasets in a folder called KMNIST.\n",
    "# The folder will be written in the same directory as this script.\n",
    "# The download will occur once. Subsequent executions will not re-download the datasets if they exist.\n",
    "kmnist_train_set = KMNIST(root='.',\n",
    "                         train=True,\n",
    "                         download=True,\n",
    "                         transform=train_transform)\n",
    "kmnist_test_set = KMNIST(root='.',\n",
    "                         train=False,\n",
    "                         download=True,\n",
    "                         transform=test_transform)\n",
    "\n",
    "# Initialize kmnist train and test data loaders. \n",
    "kmnist_train_loader = torch.utils.data.DataLoader(kmnist_train_set,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    "kmnist_test_loader = torch.utils.data.DataLoader(kmnist_test_set,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          drop_last=True)"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7NR2HHDGKAiS",
    "scrolled": true
   },
   "source": [
    "### TODO: visualize a sample image and corresponding label from KMNIST\n",
    "def matplotlib_imshow(img):\n",
    "    i, l = img\n",
    "    i = i.mean(dim=0)\n",
    "    npimg = i.numpy()\n",
    "    plt.imshow(npimg, cmap=\"Greys\")\n",
    "    print(l)\n",
    "\n",
    "matplotlib_imshow(kmnist_train_set[0])"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQe0lEQVR4nO3dfYxUVZrH8d8DNKiICtsNtg6Cji8BTdSxNZjxbR1HxERwjIr8MWENLppggjomGCUiJhqzKOOQrCKjMuDbOHF8wWh2hzUEA4kTGsMKalZebKIE7VYkCooKPPtHF6bVvucW996qunC+n6TT1fXUufex7B+3uk7de8zdBeDg16fRDQCoD8IORIKwA5Eg7EAkCDsQiX713Flzc7OPHDmynruMwueff55Y6+joyLXtESNGBOvNzc25to9idXR06LPPPrPearnCbmaXSfqTpL6SHnf3B0KPHzlypNrb2/PsEr1YvHhxYm3y5Mm5tn3XXXcF6zfccEOwbtbr7x1qpK2tLbGW+WW8mfWV9J+SxkkaLWmSmY3Ouj0AtZXnb/ZzJG1w903u/p2kv0qaUExbAIqWJ+zHSvqox88fV+77ETObambtZtbe1dWVY3cA8qj5u/HuvsDd29y9raWlpda7A5AgT9i3SBre4+dfVO4DUEJ5wr5K0klmdryZ9Zd0naQlxbQFoGiZp97cfbeZ3Szpv9U99faku79bWGf4wbZt24L12267LfO2Z8+eHawztXbwyDXP7u6vS3q9oF4A1BAflwUiQdiBSBB2IBKEHYgEYQciQdiBSNT1fHb07rvvvgvWr7766mA9dD77qFGjgmNnzJgRrNdyHj3tv3v+/PnB+mmnnRasP/bYY4m1G2+8MTj24osvDtYPRBzZgUgQdiAShB2IBGEHIkHYgUgQdiASTL2VwH333ResL1u2LFjv1y/5f+OLL74YHDtgwIBgPa/du3cn1m699dbg2BNOOCFYP//884P1m266KbE2fvz44NhVq1YF62lTmmXEkR2IBGEHIkHYgUgQdiAShB2IBGEHIkHYgUgwz14HH330UbA+Z86cXNsPXUr6lFNOybXtvEKnqX7wwQfBsQ8//HCw3tTUFKxPmJC89OCDDz4YHDtv3rxg/dFHHw3Wy4gjOxAJwg5EgrADkSDsQCQIOxAJwg5EgrADkWCevQBpl0S+6qqrgvVvvvkmWD/++OOD9bvvvjuxVuslldvb24P1UG9p8+xp8+hpjjrqqMxjN2/enGvfZZQr7GbWIekrSXsk7Xb3tiKaAlC8Io7s/+runxWwHQA1xN/sQCTyht0l/cPMVpvZ1N4eYGZTzazdzNq7urpy7g5AVnnDfp67/0rSOEnTzOyCnz7A3Re4e5u7t7W0tOTcHYCscoXd3bdUvndKeknSOUU0BaB4mcNuZgPNbNC+25IulbSuqMYAFCvPu/HDJL1UmcftJ+lZd/+vQro6wNx///3BetpcdJq5c+cG6wMHDsy1/ZDt27cH6+PGjQvWQ+eNNzc3Z2mpau6eeezGjRsL7KQcMofd3TdJOr3AXgDUEFNvQCQIOxAJwg5EgrADkSDsQCQ4xbUAaUsqp5k8eXKwnra8cB5p01PTp08P1k8++eRgPe2/rZbyTO1deumlBXZSDhzZgUgQdiAShB2IBGEHIkHYgUgQdiAShB2IBPPsVdq9e3dibfXq1bm2nTaX3adP7f5Nfvzxx4P1p556Klh/4YUXgvW+ffvud09FGT16dOaxhx12WIGdlANHdiAShB2IBGEHIkHYgUgQdiAShB2IBGEHIsE8e5W+/fbbxNrOnTuDY4cMGRKsn3rqqZl6qsb69euD9WnTpgXractF1/Jc+7zyXGJ77dq1BXZSDhzZgUgQdiAShB2IBGEHIkHYgUgQdiAShB2IBPPsVUpbujjkggsuCNb79++feduStGvXrsTa9ddfHxy7d+/eYP2ZZ54J1vv1K++v0J49ezKPHTp0aIGdlEPqkd3MnjSzTjNb1+O+IWa21MzWV74Prm2bAPKq5mX8XyRd9pP77pD0hrufJOmNys8ASiw17O7+pqRtP7l7gqRFlduLJF1ZbFsAipb1Dbph7r61cvsTScOSHmhmU82s3czau7q6Mu4OQF6534337pUBE1cHdPcF7t7m7m0tLS15dwcgo6xh/9TMWiWp8r2zuJYA1ELWsC+RtG8t3smSXimmHQC1kjpJambPSbpIUrOZfSxplqQHJP3NzKZI2izp2lo2WQah68anOf300wvs5Ofmz5+fWFu5cmVw7KxZs4L1MWPGZOqpDAYMGJB57ME4z54adneflFD6TcG9AKghPi4LRIKwA5Eg7EAkCDsQCcIORKK85yeWzMaNGzOPbWpqyrXvJUuWBOu33357Ym3UqFHBsXfccfCew/T9999nHjt27NgCOykHjuxAJAg7EAnCDkSCsAORIOxAJAg7EAnCDkSCefYqLV++PPPY1tbWYH3Tpk3B+qRJSScedgtdDnrx4sXBsYccckiwXktppw1v2bIlWD/uuOOC9c7O7NdUyTNHX1Yc2YFIEHYgEoQdiARhByJB2IFIEHYgEoQdiATz7FVasWJF5rGPPPJIsD5z5sxg/euvvw7WJ06cmFg766yzgmMbKW2552OOOSZY37p1a7C+YcOG/e5pnzIvRZ0VR3YgEoQdiARhByJB2IFIEHYgEoQdiARhByJx8E0mZrRr165g/a233sq87dWrV2ceK0nDhw8P1hcuXJhYM7Nc+26ktOvtp83Df/nll5n33afPwXccTP0vMrMnzazTzNb1uO8eM9tiZmsqX5fXtk0AeVXzz9dfJF3Wy/1/dPczKl+vF9sWgKKlht3d35S0rQ69AKihPH+Y3Gxm71Re5g9OepCZTTWzdjNr7+rqyrE7AHlkDfujkn4p6QxJWyU9lPRAd1/g7m3u3tbS0pJxdwDyyhR2d//U3fe4+15Jf5Z0TrFtAShaprCbWc9rI/9O0rqkxwIoh9R5djN7TtJFkprN7GNJsyRdZGZnSHJJHZJurF2L9bFq1apgPe2c8jwGDRoUrL/22mvB+qGHHlpkOweNPPPsO3bsKLCTckgNu7v3tkLBEzXoBUANHXwfEwLQK8IORIKwA5Eg7EAkCDsQCU5xrZg7d27Ntp12munatWuD9REjRhTZTjQ+/PDDzGMP5FODk3BkByJB2IFIEHYgEoQdiARhByJB2IFIEHYgEtHMs6ct7/vqq6/WbN9plyUePDjxql4I2Lt3b7C+cuXKzNvOc3psWXFkByJB2IFIEHYgEoQdiARhByJB2IFIEHYgEtHMs8+ZMydY37NnT832nXZudN++fWu274NZ2vN6+OGHZ9720qVLg/WJEycG6/36lS9aHNmBSBB2IBKEHYgEYQciQdiBSBB2IBKEHYhE+SYDM0q7Rvi8efPq1MnPpZ3Pzjx7Nmnz7OPHj0+sPfTQQ8GxixYtCtbTPpfxxBPhhY779+8frNdC6pHdzIab2TIze8/M3jWz6ZX7h5jZUjNbX/nOFRiAEqvmZfxuSX9w99GSxkiaZmajJd0h6Q13P0nSG5WfAZRUatjdfau7v125/ZWk9yUdK2mCpH2vdRZJurJGPQIowH69QWdmIyWdKemfkoa5+74Lu30iaVjCmKlm1m5m7V1dXXl6BZBD1WE3s8Ml/V3SLe7+o6vxubtL8t7GufsCd29z97aWlpZczQLIrqqwm1mTuoP+jLu/WLn7UzNrrdRbJXXWpkUARUiderPu+Y0nJL3v7j3XNV4iabKkByrfX6lJh1WaNWtWsF7LU1jTdL/wyV5HNjNmzEiszZ8/Pzh2586dwfrTTz8drKed4rpw4cJgvRaqmWf/taTfS1prZmsq992p7pD/zcymSNos6dqadAigEKlhd/cVkpI+vfCbYtsBUCt8XBaIBGEHIkHYgUgQdiAShB2IxAF1iuuuXbsSa88++2wdO9k/Rx55ZLDeiNMdYxD6xObLL78cHDt27NhgPW256LRLUTcCR3YgEoQdiARhByJB2IFIEHYgEoQdiARhByJxQM2zh6TNew4aNChYHzp0aLC+cePG/e5pn7TeUH9jxowJ1o844ohgffv27cH60Ucfvb8t1RxHdiAShB2IBGEHIkHYgUgQdiAShB2IBGEHInFAzbM3NTUl1mbOnBkce+214Stdd3R0BOtXXHFFsB5y3XXXBess2Vwb27ZtS6yde+65wbFp8+hTpkwJ1hu5RHgSjuxAJAg7EAnCDkSCsAORIOxAJAg7EAnCDkSimvXZh0taLGmYJJe0wN3/ZGb3SPp3SV2Vh97p7q/XqlEpPB9977335tp22nraId1L2Ce75ZZbMm8bydKuMXDJJZck1jo7O4Njly1bFqxfeOGFwXra70QjVPMbvlvSH9z9bTMbJGm1me27Av4f3f3B2rUHoCjVrM++VdLWyu2vzOx9ScfWujEAxdqvv9nNbKSkMyX9s3LXzWb2jpk9aWaDE8ZMNbN2M2vv6urq7SEA6qDqsJvZ4ZL+LukWd/9S0qOSfinpDHUf+R/qbZy7L3D3NndvC629BaC2qgq7mTWpO+jPuPuLkuTun7r7HnffK+nPks6pXZsA8koNu3W/rfiEpPfdfW6P+1t7POx3ktYV3x6AolTzbvyvJf1e0lozW1O5705Jk8zsDHVPx3VIurEG/dVNa2trsN6nT/K/i9dcc01w7Iknnpipp9jt2LEjWD/77LMzb3vFihXB+plnnpl522VVzbvxKyT1NmlY0zl1AMXiE3RAJAg7EAnCDkSCsAORIOxAJAg7EIkD6lLStTRw4MBgffbs2Ym16dOnB8eW8XTHA8Hzzz8frH/xxRfB+vLlyxNrB+M8ehqO7EAkCDsQCcIORIKwA5Eg7EAkCDsQCcIORMLcvX47M+uStLnHXc2SPqtbA/unrL2VtS+J3rIqsrcR7t7r9d/qGvaf7dys3d3bGtZAQFl7K2tfEr1lVa/eeBkPRIKwA5FodNgXNHj/IWXtrax9SfSWVV16a+jf7ADqp9FHdgB1QtiBSDQk7GZ2mZn9n5ltMLM7GtFDEjPrMLO1ZrbGzNob3MuTZtZpZut63DfEzJaa2frK917X2GtQb/eY2ZbKc7fGzC5vUG/DzWyZmb1nZu+a2fTK/Q197gJ91eV5q/vf7GbWV9IHkn4r6WNJqyRNcvf36tpIAjPrkNTm7g3/AIaZXSBph6TF7n5a5b7/kLTN3R+o/EM52N1nlKS3eyTtaPQy3pXVilp7LjMu6UpJ/6YGPneBvq5VHZ63RhzZz5G0wd03uft3kv4qaUID+ig9d39T0raf3D1B0qLK7UXq/mWpu4TeSsHdt7r725XbX0nat8x4Q5+7QF910YiwHyvpox4/f6xyrffukv5hZqvNbGqjm+nFMHffWrn9iaRhjWymF6nLeNfTT5YZL81zl2X587x4g+7nznP3X0kaJ2la5eVqKXn332BlmjutahnveullmfEfNPK5y7r8eV6NCPsWScN7/PyLyn2l4O5bKt87Jb2k8i1F/em+FXQr3zsb3M8PyrSMd2/LjKsEz10jlz9vRNhXSTrJzI43s/6SrpO0pAF9/IyZDay8cSIzGyjpUpVvKeolkiZXbk+W9EoDe/mRsizjnbTMuBr83DV8+XN3r/uXpMvV/Y78Rkl3NaKHhL5OkPS/la93G92bpOfU/bLue3W/tzFF0r9IekPSekn/I2lIiXp7StJaSe+oO1itDertPHW/RH9H0prK1+WNfu4CfdXleePjskAkeIMOiARhByJB2IFIEHYgEoQdiARhByJB2IFI/D+Xps2JIuXEPAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Q78iW7WKAiT"
   },
   "source": [
    "## 1 Feed-forward Neural network\n",
    "In this section, you will implement a simple feed-forward network from scratch. Follow the instructions/comments in each subsection to complete the general structure of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJhzKiUyKAiT"
   },
   "source": [
    "### 1.1 Activation Functions (4 + 4 + 4 pts)\n",
    "Implement [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function), [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) and [Identity](https://en.wikipedia.org/wiki/Identity_function) functions. This functions will later be used in network architecture."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NiIHSa5fKAiU"
   },
   "source": [
    "def Sigmoid(x):\n",
    "    \"\"\" Identity activation function\n",
    "    Args:\n",
    "        x (torch.tensor)\n",
    "    Return:\n",
    "        torch.tensor: a tensor of shape of x\n",
    "    \"\"\"\n",
    "    # For some reason, this formula causes accuracy to suddenly drop around epoch 3, using torch for now..\n",
    "    # return 1.0 / (1.0 +torch.exp(-x))\n",
    "    return torch.sigmoid_(x)\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    \"\"\" ReLU activation function\n",
    "    Args:\n",
    "        x (torch.tensor)\n",
    "    Return:\n",
    "        torch.tensor: a tensor of shape of x\n",
    "    \"\"\"\n",
    "\n",
    "    z = torch.zeros(x.size(), device='cuda')\n",
    "    return torch.where(x > 0, x, z)\n",
    "\n",
    "def Identity(x):\n",
    "    \"\"\" Identity activation function\n",
    "    Args:\n",
    "        x (torch.tensor)\n",
    "    Return:\n",
    "        torch.tensor: a tensor of shape of x\n",
    "    \"\"\"\n",
    "    return x"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnLM6u8aKAiU"
   },
   "source": [
    "### 1.2 Cross Entropy Loss (7.5 + 7.5 pts)\n",
    "Implement the Softmax function and Cross Entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Mv24yWSyKAiU"
   },
   "source": [
    "def Softmax(x,dim):\n",
    "    \"\"\" Softmax function\n",
    "    Args:torch.log(\n",
    "        x (torch.tensor): inputs tensor of size (B,F)\n",
    "        dim (int): A dimension along which Softmax will be computed\n",
    "    Return:\n",
    "        torch.tensor: a tensor of shape of x\n",
    "    \"\"\"\n",
    "    mat = torch.nn.Softmax(dim)\n",
    "    return mat(x)\n",
    "\n",
    "\n",
    "def CE_loss(predictions,labels):\n",
    "    \"\"\" Cross entropy loss\n",
    "    Args:\n",
    "        predictions (torch.tensor): tensor of shape of (B,C)\n",
    "        labels (torch.tensor): tensor of shape of (B,1)\n",
    "    Returns:\n",
    "        torch.tensor: a tensor of shape of (1,)\n",
    "    \"\"\"\n",
    "\n",
    "    ### TODO: Fill out this function\n",
    "    loss = nn.CrossEntropyLoss().cuda()\n",
    "    output = loss(predictions, labels)\n",
    "    return output\n"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tJxocVAKAiV"
   },
   "source": [
    "### 1.3 Network (2.5 + 7.5 pts)\n",
    "Complete the code for the simple feed-forward network shown below. \n",
    "\n",
    "Network parameters will be stored in a dictionary called `params`. weights and biases keys appear in the format `W#` and `b#` where `#` indicates the layer number. For example, the weights and biases of the first layer are `W1` and `b1`. Weights and biases are initialized inside `init_weights` function.\n",
    "\n",
    "**Notes:** \n",
    "1. Set `requires_grad=True` when initializing weights and biases to have [pytorch automatic differentiation engine](https://pytorch.org/docs/stable/autograd.html) calculate the gradients.\n",
    "2. Assign network parameters and inputs on the same `device`.\n",
    "3. Initialize weights and biases with samples from normal distribution with mean 0 and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u_BV_J1NKAiW"
   },
   "source": [
    "params = {}\n",
    "\n",
    "class my_nn:\n",
    "    def __init__(self, layers_dim, layers_activation=None, device='cuda'):\n",
    "        \"\"\" Initialize network\n",
    "        Args:\n",
    "            layers_dims (List of ints): list of Size of each layer of the network\n",
    "                                        [inputs,layer1,...,outputs]\n",
    "            layers_activation (List of strings): list of activation function for each hidden layer\n",
    "                                        of the network[layer1,...,outputs]\n",
    "            device (str): a device that will be used for computation\n",
    "                Default: 'cpu'\n",
    "\n",
    "        \"\"\"\n",
    "        self.layers_activation = layers_activation\n",
    "        self.params = {}\n",
    "        self.num_layers = len(layers_dim) - 1\n",
    "        self.layers_dim = layers_dim\n",
    "        self.device = device\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\" Initialize weights and biases of network based on layers dimension.\n",
    "            Store weights and biases in self.params.\n",
    "            weights and biases key should be of format \"W#\" and \"b#\" where # is the layer number.\n",
    "            Example: for layer 1, weight and bias key is \"W1\" and \"b1\"\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        ### TODO: Initialize weights and bias of network\n",
    "        ### TODO: Store weights and biases in self.params\n",
    "        self.params = {\n",
    "            'W1': torch.randn((784, 512), requires_grad=True, device=\"cuda\"),\n",
    "            'b1': torch.zeros((1, 512), requires_grad=True, device=\"cuda\"),\n",
    "            'W2': torch.randn((512, 512), requires_grad=True, device=\"cuda\"),\n",
    "            'b2': torch.zeros((1, 512), requires_grad=True, device=\"cuda\"),\n",
    "            'W3': torch.randn((512, 10), requires_grad=True, device=\"cuda\"),\n",
    "            'b3': torch.zeros((1, 10), requires_grad=True, device=\"cuda\")\n",
    "        }\n",
    "        ### HINT: Remember to set require_grad to True\n",
    "        ### HINT: Remember to put tensors of target device\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Perform forward pass\n",
    "        Args:\n",
    "            x (torch.tensor): tensor of shape of (B, C, H, W)\n",
    "\n",
    "        Return:\n",
    "            torch.tensor: tensor of shape of (B, N_classes)\n",
    "        \"\"\"\n",
    "        ### TODO: Fill out this function\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = torch.mm(x, self.params['W1'])\n",
    "        x = x + self.params['b1']\n",
    "        x = Sigmoid(x)\n",
    "\n",
    "        x = torch.mm(x, self.params['W2'])\n",
    "        x = x + self.params['b2']\n",
    "        x = Sigmoid(x)\n",
    "\n",
    "        x = torch.mm(x, self.params['W3'])\n",
    "        x = x + self.params['b3']\n",
    "        x = Sigmoid(x)\n",
    "\n",
    "        return x\n"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzH7h-aDKAiY"
   },
   "source": [
    "### 1.4 Training the network (12+12+6 pts)\n",
    "Complete and run the following cells to train the network. You can use the predefined network hyper parameters or try your own."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0XCEZfcKKAia"
   },
   "source": [
    "def calc_accuracy(predictions, labels):\n",
    "    classes = torch.argmax(predictions, dim=1)\n",
    "    return (classes == labels).float().sum()\n",
    "\n",
    "def Train(model, optimizer, dataloader, device):\n",
    "    \"\"\" performs training on train set\n",
    "    Args:\n",
    "        model (my_nn instance): model to be trained\n",
    "        optimizer (torch.optim instance)\n",
    "        dataloader (torch.utils.data.DataLoader instance): dataloader for train set\n",
    "        device (str): computation device ['cpu','cuda',...]\n",
    "    Returns:\n",
    "        list of floats: mini_batch loss sampled every 20 steps for visualization purposes\n",
    "        list of floats: mini_batch accuracy sampled every 20 steps for visualization purposes\n",
    "    \"\"\"\n",
    "    loss_tracker = []\n",
    "    accuracy_tracker = []\n",
    "    for i, (data, label) in enumerate(dataloader):\n",
    "        ### TODO: Put data and label on target device\n",
    "        data = data.to(device).to(torch.float32)\n",
    "        labels = label.to(device)\n",
    "        ### TODO: Set gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        ### TODO: Pass data to the model\n",
    "        out = model.forward(data)\n",
    "        ### TODO: Calculate the loss of predicted labels vs ground truth labels\n",
    "        loss = CE_loss(out, labels)\n",
    "        ### TODO: Calculate gradients and update weights and biases\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 20:\n",
    "            with torch.no_grad():\n",
    "                loss_tracker.append(loss.item())\n",
    "                ### TODO: calculate accuracy of mini_batch\n",
    "                accuracy = calc_accuracy(out, labels)\n",
    "                accuracy_tracker.append(accuracy / data.size(0))\n",
    "\n",
    "    return loss_tracker, accuracy_tracker\n",
    "\n",
    "\n",
    "def Test(model, dataloader, device):\n",
    "    \"\"\" performs training on train set\n",
    "    Args:\n",
    "        model (my_nn instance): model to be trained\n",
    "        dataloader (torch.utils.data.DataLoader instance)\n",
    "        device (str): computation device ['cpu','cuda',...]\n",
    "    Returns:\n",
    "        floats: test set loss for visualization purposes\n",
    "        floats: test set accuracy for visualization purposes\n",
    "    \"\"\"\n",
    "    loss_tracker = []\n",
    "    accuracy_tracker = []\n",
    "    for i, (data, label) in enumerate(dataloader):\n",
    "        ### TODO: Put data and label on target device\n",
    "        data = data.to(device).to(torch.float32)\n",
    "        labels = label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ### TODO: Pass data to the model\n",
    "            out = model.forward(data)\n",
    "            ### TODO: Calculate the loss of predicted labels vs ground truth labels\n",
    "            loss = CE_loss(out, labels)\n",
    "\n",
    "            ### TODO: calculate accuracy of mini_batch\n",
    "            accuracy = calc_accuracy(out, labels)\n",
    "\n",
    "        loss_tracker.append(loss.item())\n",
    "        accuracy_tracker.append(accuracy / data.size(0))\n",
    "\n",
    "    return sum(loss_tracker) / len(loss_tracker), sum(accuracy_tracker) / len(accuracy_tracker)\n",
    "        "
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MRgQgscVKAib"
   },
   "source": [
    "# Training hyper parameters\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "layers_dim = [28*28,512,512,10]\n",
    "\n",
    "### TODO: Set target device for computations\n",
    "device = 'cuda'\n",
    "print(f'device: {device}')\n",
    "\n",
    "### TODO: Initialize model using layers_dim\n",
    "model = my_nn(layers_dim, layers_activation='sigmoid', device=device)\n",
    "\n",
    "### TODO: Initialize Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.params.values(), lr=learning_rate)\n",
    "\n",
    "train_loss_tracker = []\n",
    "train_accuracy_tracker = []\n",
    "\n",
    "test_loss_tracker = []\n",
    "test_accuracy_tracker = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    train_loss,train_accuracy = Train(model,optimizer,kmnist_train_loader,device)\n",
    "    test_loss , test_accuracy = Test(model,kmnist_test_loader,device)\n",
    "    train_loss_tracker.extend(train_loss)\n",
    "    train_accuracy_tracker.extend(train_accuracy)\n",
    "    test_loss_tracker.append(test_loss)\n",
    "    test_accuracy_tracker.append(test_accuracy)\n",
    "    print('\\t training loss/accuracy: {0:.2f}/{1:.2f}'.format(sum(train_loss)/len(train_loss), sum(train_accuracy)/len((train_accuracy))))\n",
    "    print('\\t testing loss/accuracy: {0:.2f}/{1:.2f}'.format(test_loss, test_accuracy))"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Epoch: 0\n",
      "\t training loss/accuracy: 1.78/0.66\n",
      "\t testing loss/accuracy: 1.83/0.62\n",
      "Epoch: 1\n",
      "\t training loss/accuracy: 1.64/0.81\n",
      "\t testing loss/accuracy: 1.79/0.66\n",
      "Epoch: 2\n",
      "\t training loss/accuracy: 1.61/0.84\n",
      "\t testing loss/accuracy: 1.76/0.69\n",
      "Epoch: 3\n",
      "\t training loss/accuracy: 1.59/0.86\n",
      "\t testing loss/accuracy: 1.74/0.71\n",
      "Epoch: 4\n",
      "\t training loss/accuracy: 1.57/0.88\n",
      "\t testing loss/accuracy: 1.71/0.73\n",
      "Epoch: 5\n",
      "\t training loss/accuracy: 1.56/0.89\n",
      "\t testing loss/accuracy: 1.70/0.74\n",
      "Epoch: 6\n",
      "\t training loss/accuracy: 1.55/0.90\n",
      "\t testing loss/accuracy: 1.69/0.75\n",
      "Epoch: 7\n",
      "\t training loss/accuracy: 1.55/0.91\n",
      "\t testing loss/accuracy: 1.69/0.75\n",
      "Epoch: 8\n",
      "\t training loss/accuracy: 1.54/0.91\n",
      "\t testing loss/accuracy: 1.68/0.77\n",
      "Epoch: 9\n",
      "\t training loss/accuracy: 1.54/0.92\n",
      "\t testing loss/accuracy: 1.68/0.77\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ac_LFfwJKAib",
    "scrolled": true
   },
   "source": [
    "### TODO: visualize train_loss and train_accuracy\n",
    "### TODO: visualize test_loss and test_accuracy\n"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIo6qu1jKAic"
   },
   "source": [
    "## 2 Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZayT7BO8QNV2"
   },
   "source": [
    "Implement a CNN and train your network. Using GPU will dramatically increase the training speed. If you are planning on using [Google Colab](https://colab.research.google.com/), one way to load the dataset is to load it from your google drive. Run the cell below to mount your google drive storage. If you are training on your local machine, comment out the following two lines."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wn7FtKW4Ln1b"
   },
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ce6IrzbBKAic"
   },
   "source": [
    "In this section, we will use the [Stanford Dogs Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/). You only need download [images](http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar) (Extract and upload this folder to your google drive if you are planning to use google colab). \n",
    "\n",
    "Run the cells below to load and visualize this dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uXSRH8w1cIfZ"
   },
   "source": [
    "# load dataset from path\n",
    "# set path to images location on your local machine or google drive\n",
    "#path = 'drive/MyDrive/images/Images'  # Google drive\n",
    "path = './images/Images'               #local machine\n",
    "dataset = ImageFolder(path)\n",
    "print(f'number of images: {len(dataset)}')\n",
    "print(f'number of classes: {len(dataset.classes)}')"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './images/Images'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-12-cd22c3bc5962>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m#path = 'drive/MyDrive/images/Images'  # Google drive\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mpath\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'./images/Images'\u001B[0m               \u001B[1;31m#local machine\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mdataset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mImageFolder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'number of images: {len(dataset)}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'number of classes: {len(dataset.classes)}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mg:\\users\\colin\\school\\w21\\comp425\\project\\comp425_project\\venv\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001B[0m\n\u001B[0;32m    251\u001B[0m             \u001B[0mis_valid_file\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mOptional\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mCallable\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    252\u001B[0m     ):\n\u001B[1;32m--> 253\u001B[1;33m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n\u001B[0m\u001B[0;32m    254\u001B[0m                                           \u001B[0mtransform\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    255\u001B[0m                                           \u001B[0mtarget_transform\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtarget_transform\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mg:\\users\\colin\\school\\w21\\comp425\\project\\comp425_project\\venv\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001B[0m\n\u001B[0;32m    124\u001B[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001B[0;32m    125\u001B[0m                                             target_transform=target_transform)\n\u001B[1;32m--> 126\u001B[1;33m         \u001B[0mclasses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclass_to_idx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_find_classes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mroot\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    127\u001B[0m         \u001B[0msamples\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmake_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mroot\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclass_to_idx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mextensions\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mis_valid_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    128\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msamples\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mg:\\users\\colin\\school\\w21\\comp425\\project\\comp425_project\\venv\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001B[0m in \u001B[0;36m_find_classes\u001B[1;34m(self, dir)\u001B[0m\n\u001B[0;32m    162\u001B[0m             \u001B[0mNo\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[1;32mis\u001B[0m \u001B[0ma\u001B[0m \u001B[0msubdirectory\u001B[0m \u001B[0mof\u001B[0m \u001B[0manother\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    163\u001B[0m         \"\"\"\n\u001B[1;32m--> 164\u001B[1;33m         \u001B[0mclasses\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0md\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0md\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mscandir\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdir\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0md\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_dir\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    165\u001B[0m         \u001B[0mclasses\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msort\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    166\u001B[0m         \u001B[0mclass_to_idx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[0mcls_name\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcls_name\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclasses\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] The system cannot find the path specified: './images/Images'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aPe_cqDPcIfa"
   },
   "source": [
    "# Create train and test splits of original dataset\n",
    "test_pct = 0.3\n",
    "test_size = int(len(dataset)*test_pct)\n",
    "train_size = len(dataset) - test_size\n",
    "train_ds, test_ds = random_split(dataset, [train_size, test_size])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t6WlRHFxOw73"
   },
   "source": [
    "# In order to apply transformations, we use a custom dataset\n",
    "# see https://pytorch.org/docs/stable/data.html#iterable-style-datasets\n",
    "class DogBreedDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ds, transform=None):\n",
    "        self.ds = ds\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.ds[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)  \n",
    "            return img, label"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Cmm9TWo_cIfb"
   },
   "source": [
    "batch_size =64\n",
    "\n",
    "#train set transforms\n",
    "train_transform = transforms.Compose([\n",
    "   transforms.Resize((240, 240)),\n",
    "    transforms.ToTensor()    \n",
    "])\n",
    "\n",
    "# test set transforms\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((240,240)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Initialize train and test sets\n",
    "train_dataset = DogBreedDataset(train_ds, train_transform)\n",
    "test_dataset = DogBreedDataset(test_ds, test_transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dl = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f5N0-ApkcIfc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def show_batch(dl):\n",
    "    for img, lb in dl:\n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(img.cpu(), nrow=16).permute(1,2,0))\n",
    "        break\n",
    "show_batch(train_dl)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}